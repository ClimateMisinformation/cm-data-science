{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2621486",
   "metadata": {},
   "source": [
    "# Can we detect climate misinformation by analysing a single word or phrase?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea86449a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, '../text_preprocessing/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d10c543f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from preprocessing import *\n",
    "from embeddings import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc75216",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../text_preprocessing/preprocessing.py:100: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_encoded['human_label'] = encoded_classes\n",
      "../text_preprocessing/preprocessing.py:103: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_encoded['human_binary_label'] = df_encoded['human_label'].apply(lambda label: 1 if label > 0 else 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing data...\n",
      "   label                                               text\n",
      "0    118  Buenos Aires Playa is an urban beach on the Rí...\n",
      "1    123  Contributed by Robert Lyman  © 2017The United ...\n",
      "2    119  Contributed by Robert Lyman  © 2017The United ...\n",
      "3    119  Share this...FacebookTwitterState parliamentar...\n",
      "4    122  [CHECK OUT OUR PLANS](https://thebfd.co.nz/sub...\n",
      "Size of dataframe\n",
      "Index(['label', 'text'], dtype='object')\n",
      "(778, 2)\n",
      "\n",
      "Dropping na values..\n",
      "NaN values per column\n",
      "label     0\n",
      "text     62\n",
      "dtype: int64\n",
      "\n",
      "Encoding classes..\n",
      "118 is encoded to [0]\n",
      "119 is encoded to [1]\n",
      "120 is encoded to [2]\n",
      "\n",
      "Starting text preprocessing..\n"
     ]
    }
   ],
   "source": [
    "path = '../labelled_data/labelled_data_14032021.csv'\n",
    "\n",
    "print(\"Importing data...\")\n",
    "df = import_data(path)\n",
    "\n",
    "print(\"\\nDropping na values..\")\n",
    "df = na_values(df)\n",
    "\n",
    "print(\"\\nEncoding classes..\")\n",
    "df, label_columns = class_encoding(df)\n",
    "\n",
    "# print(\"\\nFiltering long doocuments and exploring length..\")\n",
    "# documents_len_exploration_and_filter(df,max_len=1500)\n",
    "\n",
    "print(\"\\nStarting text preprocessing..\")\n",
    "clean_text = preprocessing(df)\n",
    "\n",
    "df['clean_text'] = clean_text\n",
    "\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139167c1",
   "metadata": {},
   "source": [
    "## Wordclouds for each Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c807ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same code can be used to generate wordclouds of other categories.\n",
    "def plot_wordcloud(label):\n",
    "\n",
    "    plt.figure(figsize=(35,20))\n",
    "\n",
    "#     text = df[df.human_label == label].text.fillna(\"\").values\n",
    "    full_text = \"\"\n",
    "    for doc in df[df.human_label == label].clean_text.to_list():\n",
    "        for text in doc:\n",
    "            full_text = \"{0} {1}\".format(full_text,text).strip()\n",
    "    \n",
    "    cloud_text = WordCloud(\n",
    "                              stopwords=STOPWORDS,\n",
    "                              background_color='black',\n",
    "                              collocations=False,\n",
    "                              width=2000,\n",
    "                              height=1000\n",
    "                             ).generate(full_text) #\" \".join(text))\n",
    "    plt.axis('off')\n",
    "    plt.title('Word cloud for \"{0}\"\\n'.format(label),fontsize=40)\n",
    "    plt.imshow(cloud_text)\n",
    "    plt.show()\n",
    "\n",
    "for col in df.human_label.unique():\n",
    "#     print(col)\n",
    "    plot_wordcloud(col)\n",
    "    print(\"\\n\\n#########\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292d9a23",
   "metadata": {},
   "source": [
    "## Most frequent Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e916d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_n = []\n",
    "n = 500\n",
    "\n",
    "for col in df.human_label.unique():\n",
    "    print(col)\n",
    "    words = []\n",
    "    for doc in df.loc[df.human_label == col, 'clean_text'].to_list():\n",
    "        words += doc\n",
    "\n",
    "    d = {\n",
    "        \"label\": col,\n",
    "        \"top_{0}\".format(n): pd.Series(words).value_counts()[:n].index\n",
    "    }\n",
    "    top_n.append(d)\n",
    "\n",
    "top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20044bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_analysis(word_list, min_length=4, min_freq=0.1, max_freq_other=0.1):\n",
    "    i = 0\n",
    "\n",
    "    # min_length = 4\n",
    "    # min_freq = 0.1\n",
    "    # max_freq_other = 0.1\n",
    "\n",
    "    for word in sorted(word_list):\n",
    "        if len(word) < min_length:\n",
    "            continue\n",
    "\n",
    "    #     freq = df.loc[df.text.fillna(\"\").str.contains(word, regex=False, case=False).fillna(False), \"human_label\"].value_counts() / df.human_label.value_counts()\n",
    "        mask = df.clean_text.apply(lambda x: word in x)\n",
    "        freq = df.loc[mask, \"human_label\"].value_counts() / df.human_label.value_counts()\n",
    "        if (freq[0] < min_freq) or (freq[1] > max_freq_other) and (freq[2] > max_freq_other):\n",
    "            continue\n",
    "\n",
    "        i += 1\n",
    "        print(\"\\n\\n##### {0}\\n\".format(word))\n",
    "        print(freq)\n",
    "\n",
    "    print(\"\\n\\n##### {0} words selected for analysis\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86967584",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_analysis(top_n[0][\"top_{0}\".format(n)], min_freq=0.3, max_freq_other=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4b0daf",
   "metadata": {},
   "source": [
    "## Most frequent Keywords (misinfo focus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57509ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_misinfo_only = []\n",
    "\n",
    "for val in top_n[0][\"top_{0}\".format(n)]:\n",
    "    if (val in top_n[1][\"top_{0}\".format(n)]) or (val in top_n[2][\"top_{0}\".format(n)]):\n",
    "        continue\n",
    "    top_misinfo_only.append(val)\n",
    "\n",
    "len(top_misinfo_only), top_misinfo_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70679a1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# i = 0\n",
    "\n",
    "# min_length = 4\n",
    "# min_freq = 0.1\n",
    "# max_freq_other = 0.1\n",
    "\n",
    "# for word in sorted(top_misinfo_only):\n",
    "#     if len(word) < min_length:\n",
    "#         continue\n",
    "    \n",
    "# #     freq = df.loc[df.text.fillna(\"\").str.contains(word, regex=False, case=False).fillna(False), \"human_label\"].value_counts() / df.human_label.value_counts()\n",
    "#     mask = df.clean_text.apply(lambda x: word in x)\n",
    "#     freq = df.loc[mask, \"human_label\"].value_counts() / df.human_label.value_counts()\n",
    "#     if (freq[0] < min_freq) or (freq[1] > max_freq_other) and (freq[2] > max_freq_other):\n",
    "#         continue\n",
    "        \n",
    "#     i += 1\n",
    "#     print(\"\\n\\n##### {0}\\n\".format(word))\n",
    "#     print(freq)\n",
    "    \n",
    "# print(\"\\n\\n##### {0} words selected for analysis\".format(i))\n",
    "\n",
    "word_analysis(top_misinfo_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238462d6",
   "metadata": {},
   "source": [
    "## Curated Keywords from Notion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04a8c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "notion_words_misinfo = [\"climatecult\", \"climatehysteria\", \"globalcooling\", \"climatescam\", \"climatehoax\", \"climatechangehoax\",\n",
    "                        \"greenwashing\"]\n",
    "\n",
    "notion_words_climate = [\"TheTimeIsNow\", \"weneedexperts\", \"ecology\", \"climatechange\", \"HowtoSavethePlanet\", \"VoiceForThePlanet\",\n",
    "                        \"FightForYourWorld\", \"connect2earth\", \"globalwarming\"]\n",
    "\n",
    "notion_words_notrel = [\"coronavirus\"]\n",
    "\n",
    "notion_keywords = [\n",
    "    {\"label\": \"Climate Misinformation\", \"words\": notion_words_misinfo},\n",
    "    {\"label\": \"Climate Related\", \"words\": notion_words_climate},\n",
    "    {\"label\": \"Not Climate Related\", \"words\": notion_words_notrel},\n",
    "]\n",
    "\n",
    "for k in notion_keywords:\n",
    "    print(\"\\n-------------------------------\\n{0}\\n{1}\".format(k[\"label\"], k[\"words\"]))\n",
    "\n",
    "# df.loc[df.text.fillna(\"\").str.contains(\"({0})\".format(\"|\".join(notion_words_misinfo), case=False)), \"human_label\"].value_counts() / df.human_label.value_counts()\n",
    "\n",
    "#     for word in k[\"words\"]:\n",
    "# #         freq = df.loc[df.text.fillna(\"\").str.contains(word, regex=False, case=False).fillna(False), \"human_label\"].value_counts() / df.human_label.value_counts()\n",
    "#         mask = df.clean_text.apply(lambda x: word in x)\n",
    "#         freq = df.loc[mask, \"human_label\"].value_counts() / df.human_label.value_counts()\n",
    "#         print(\"\\n\\n##### {0}\\n\".format(word))\n",
    "#         print(freq)\n",
    "\n",
    "    word_analysis(k[\"words\"], min_freq=0, max_freq_other=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2436fcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e5b0d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
